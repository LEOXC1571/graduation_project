
@article{zhou_deep_2018,
  title    = {Deep {Interest} {Network} for {Click}-{Through} {Rate} {Prediction}},
  url      = {http://arxiv.org/abs/1706.06978},
  abstract = {Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding\&MLP methods to capture user’s diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.},
  language = {en},
  urldate  = {2021-12-06},
  journal  = {arXiv:1706.06978 [cs, stat]},
  author   = {Zhou, Guorui and Song, Chengru and Zhu, Xiaoqiang and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  month    = sep,
  year     = {2018},
  note     = {arXiv: 1706.06978},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, H.3.2},
  file     = {Zhou 等。 - 2018 - Deep Interest Network for Click-Through Rate Predi.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/XND9PGT5/Zhou 等。 - 2018 - Deep Interest Network for Click-Through Rate Predi.pdf:application/pdf}
}

@article{cn2022w,
  title    = {内容供给侧信息过载问题及优化策略——以互联网内容平台为例},
  volume   = {7},
  issn     = {2096-1162},
  url      = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDAUTO&filename=KJQB202209006&uniplatform=NZKPT&v=g6Aat-oYyJdyMGCAAK-NFT5t76naESxqQrepX64ps6OfTymEIUXhoQDo_ZJE5caY},
  abstract = {在互联网内容信息爆炸式增长背景下，为缓解信息过载问题给人们带来的诸多负面影响，除了倡导提高用户自身的信息素养以外，对内容供给侧进行优化是一种更为现实和有效的策略。结合对信息过载研究现状的梳理，以互联网内容平台为例，分析了互联网内容平台信息过载问题的产生原因和负面影响，发现内容平台的信息过剩、信息重复和用户的信息强迫症是导致互联网内容平台信息过载发生的主要原因，解决互联网内容平台信息过载问题需要更加重视内容供给侧的优化，由此结合对知乎这一代表性的互联网内容平台案例的分析，提出了3方面的优化策略，包括以用户信息为支撑形成精准用户画像、以内容信息为核心建立分类过滤机制和以内容算法为基础构建推荐和搜索...},
  language = {中文;},
  number   = {09},
  urldate  = {2023-01-19},
  journal  = {图书情报导刊},
  author   = {王, 梦琪 and 唐, 长乐},
  year     = {2022},
  keywords = {information overload, Internet content platform, optimization strategy, 互联网内容平台, 优化策略, 信息过载},
  pages    = {30--37},
  file     = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/HCDNY2X5/王 and 唐 - 2022 - 内容供给侧信息过载问题及优化策略——以互联网内容平台为例.pdf:application/pdf}
}

@article{cn2022y,
  title    = {重新认识“信息茧房”——智媒时代工具理性与价值理性的共生机制研究},
  issn     = {1002-2295},
  url      = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2022&filename=XWXZ202203008&uniplatform=NZKPT&v=ObYTAKehVRiYMcwO8Q1q2QoFV8k6lsPHKijudN7-7ZnI6YwSA8R0fL-pTgNSVAyw},
  abstract = {智能传播时代，以人工智能为核心的算法推荐技术得到了广泛应用，但随之成为隐忧的，还有信息茧房效应。本文阐释了当前对信息茧房效应的片面解读，并基于信息效率的工具理性扩张和技术反思的价值理性回归视角，丰富了信息茧房效应的概念维度。通过对典型算法推荐媒体平台知乎用户的配额分层抽样调查，本文考察了使用时间、媒介环境、媒介素养、社会资本和媒介期望五个维度对信息茧房效应的影响机制，拟合成为“TELCE整合模型”。结果表明，算法媒介使用时间越久，信息茧房效应反而降低；在媒介环境、媒介素养、媒介期望等维度上，信息茧房呈现出工具理性和价值理性共存共生的现象，算法知识效能感与信息茧房三个层次变量的负相关性进一步暗示...},
  language = {中文;},
  number   = {03},
  urldate  = {2023-01-19},
  journal  = {新闻与写作},
  author   = {虞, 鑫 and 王, 金鹏},
  year     = {2022},
  keywords = {TELCE整合模型, 价值理性, 信息茧房, 工具理性},
  pages    = {65--78},
  file     = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/B696GQK2/虞 and 王 - 2022 - 重新认识“信息茧房”——智媒时代工具理性与价值理性的共生机制研究.pdf:application/pdf}
}

@inproceedings{covington_deep_2016,
  address   = {New York, NY, USA},
  series    = {{RecSys} '16},
  title     = {Deep {Neural} {Networks} for {YouTube} {Recommendations}},
  isbn      = {978-1-4503-4035-9},
  url       = {https://doi.org/10.1145/2959100.2959190},
  doi       = {10.1145/2959100.2959190},
  abstract  = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.},
  urldate   = {2023-01-19},
  booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
  publisher = {Association for Computing Machinery},
  author    = {Covington, Paul and Adams, Jay and Sargin, Emre},
  month     = sep,
  year      = {2016},
  keywords  = {deep learning, recommender system, scalability},
  pages     = {191--198},
  file      = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/PAXSG2FG/Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:application/pdf}
}

@misc{wang_billion-scale_2018,
  title     = {Billion-scale {Commodity} {Embedding} for {E}-commerce {Recommendation} in {Alibaba}},
  url       = {http://arxiv.org/abs/1803.02349},
  doi       = {10.48550/arXiv.1803.02349},
  abstract  = {Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. The billion-scale data in Taobao creates three major challenges to Taobao's RS: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on the graph embedding framework. We first construct an item graph from users' behavior history. Each item is then represented as a vector using graph embedding. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using online A/B test, we show that the online Click-Through-Rate (CTRs) are improved comparing to the previous recommendation methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao's live production environment.},
  urldate   = {2023-01-19},
  publisher = {arXiv},
  author    = {Wang, Jizhe and Huang, Pipei and Zhao, Huan and Zhang, Zhibo and Zhao, Binqiang and Lee, Dik Lun},
  month     = may,
  year      = {2018},
  note      = {arXiv:1803.02349 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
  file      = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/UDKDH9J6/Wang et al. - 2018 - Billion-scale Commodity Embedding for E-commerce R.pdf:application/pdf}
}

@misc{liu_monolith_2022,
  title      = {Monolith: {Real} {Time} {Recommendation} {System} {With} {Collisionless} {Embedding} {Table}},
  shorttitle = {Monolith},
  url        = {http://arxiv.org/abs/2209.07663},
  abstract   = {Building a scalable and real-time recommendation system is vital for many businesses driven by time-sensitive customer feedback, such as short-videos ranking or online ads. Despite the ubiquitous adoption of production-scale deep learning frameworks like TensorFlow or PyTorch, these general-purpose frameworks fall short of business demands in recommendation scenarios for various reasons: on one hand, tweaking systems based on static parameters and dense computations for recommendation with dynamic and sparse features is detrimental to model quality; on the other hand, such frameworks are designed with batch-training stage and serving stage completely separated, preventing the model from interacting with customer feedback in real-time. These issues led us to reexamine traditional approaches and explore radically different design choices. In this paper, we present Monolith, a system tailored for online training. Our design has been driven by observations of our application workloads and production environment that reflects a marked departure from other recommendations systems. Our contributions are manifold: first, we crafted a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint; second, we provide an production-ready online training architecture with high fault-tolerance; finally, we proved that system reliability could be traded-off for real-time learning. Monolith has successfully landed in the BytePlus Recommend product.},
  urldate    = {2023-01-19},
  publisher  = {arXiv},
  author     = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.07663 [cs]},
  keywords   = {Computer Science - Information Retrieval},
  file       = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/E6TJ3ELC/Liu et al. - 2022 - Monolith Real Time Recommendation System With Col.pdf:application/pdf}
}

@inproceedings{grbovic_real-time_2018,
  address   = {New York, NY, USA},
  series    = {{KDD} '18},
  title     = {Real-time {Personalization} using {Embeddings} for {Search} {Ranking} at {Airbnb}},
  isbn      = {978-1-4503-5552-0},
  url       = {https://doi.org/10.1145/3219819.3219885},
  doi       = {10.1145/3219819.3219885},
  abstract  = {Search Ranking and Recommendations are fundamental problems of crucial interest to major Internet companies, including web search engines, content publishing websites and marketplaces. However, despite sharing some common characteristics a one-size-fits-all solution does not exist in this space. Given a large difference in content that needs to be ranked, personalized and recommended, each marketplace has a somewhat unique challenge. Correspondingly, at Airbnb, a short-term rental marketplace, search and recommendation problems are quite unique, being a two-sided marketplace in which one needs to optimize for host and guest preferences, in a world where a user rarely consumes the same item twice and one listing can accept only one guest for a certain set of dates. In this paper we describe Listing and User Embedding techniques we developed and deployed for purposes of Real-time Personalization in Search Ranking and Similar Listing Recommendations, two channels that drive 99\% of conversions. The embedding models were specifically tailored for Airbnb marketplace, and are able to capture guest's short-term and long-term interests, delivering effective home listing recommendations. We conducted rigorous offline testing of the embedding models, followed by successful online tests before fully deploying them into production.},
  urldate   = {2023-01-19},
  booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
  publisher = {Association for Computing Machinery},
  author    = {Grbovic, Mihajlo and Cheng, Haibin},
  month     = jul,
  year      = {2018},
  keywords  = {personalization, search ranking, user modeling},
  pages     = {311--320},
  file      = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/ESFWQHTS/Grbovic and Cheng - 2018 - Real-time Personalization using Embeddings for Sea.pdf:application/pdf}
}

@misc{yin_challenging_2012,
  title     = {Challenging the {Long} {Tail} {Recommendation}},
  url       = {http://arxiv.org/abs/1205.6700},
  doi       = {10.48550/arXiv.1205.6700},
  abstract  = {The success of "infinite-inventory" retailers such as Amazon.com and Netflix has been largely attributed to a "long tail" phenomenon. Although the majority of their inventory is not in high demand, these niche products, unavailable at limited-inventory competitors, generate a significant fraction of total revenue in aggregate. In addition, tail product availability can boost head sales by offering consumers the convenience of "one-stop shopping" for both their mainstream and niche tastes. However, most of existing recommender systems, especially collaborative filter based methods, can not recommend tail products due to the data sparsity issue. It has been widely acknowledged that to recommend popular products is easier yet more trivial while to recommend long tail products adds more novelty yet it is also a more challenging task. In this paper, we propose a novel suite of graph-based algorithms for the long tail recommendation. We first represent user-item information with undirected edge-weighted graph and investigate the theoretical foundation of applying Hitting Time algorithm for long tail item recommendation. To improve recommendation diversity and accuracy, we extend Hitting Time and propose efficient Absorbing Time algorithm to help users find their favorite long tail items. Finally, we refine the Absorbing Time algorithm and propose two entropy-biased Absorbing Cost algorithms to distinguish the variation on different user-item rating pairs, which further enhances the effectiveness of long tail recommendation. Empirical experiments on two real life datasets show that our proposed algorithms are effective to recommend long tail items and outperform state-of-the-art recommendation techniques.},
  urldate   = {2023-01-19},
  publisher = {arXiv},
  author    = {Yin, Hongzhi and Cui, Bin and Li, Jing and Yao, Junjie and Chen, Chen},
  month     = may,
  year      = {2012},
  note      = {arXiv:1205.6700 [cs]},
  keywords  = {Computer Science - Databases},
  file      = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/8CW7MJY4/Yin et al. - 2012 - Challenging the Long Tail Recommendation.pdf:application/pdf}
}

@article{chen_airec_2021,
  title        = {{AIRec}: Attentive intersection model for tag-aware recommendation},
  volume       = {421},
  issn         = {0925-2312},
  url          = {https://www.sciencedirect.com/science/article/pii/S0925231220312789},
  doi          = {10.1016/j.neucom.2020.08.018},
  shorttitle   = {{AIRec}},
  pages        = {105--114},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  author       = {Chen, Bo and Ding, Yue and Xin, Xin and Li, Yunzhe and Wang, Yule and Wang, Dong},
  urldate      = {2022-05-09},
  date         = {2021-01-15},
  langid       = {english},
  keywords     = {Attention mechanism, Neural networks, Tag-aware collaborative filtering}
}

@article{zuo_tag-aware_2016,
  title    = {Tag-aware recommender systems based on deep neural networks},
  volume   = {204},
  issn     = {09252312},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216301151},
  doi      = {10.1016/j.neucom.2015.10.134},
  abstract = {Many researchers have introduced tag information to recommender systems to improve the performance of traditional recommendation techniques. However, user-deﬁned tags will usually suffer from many problems, such as sparsity, redundancy, and ambiguity. To address these problems, we propose a new recommendation algorithm based on deep neural networks. In the proposed algorithm, users' proﬁles are initially represented by tags and then a deep neural network model is used to extract the in-depth features from tag space layer by layer. In this way, representations of the raw data will become more abstract and advanced, and therefore the unique structure of tag space will be revealed automatically. Based on those extracted abstract features, users' proﬁles are updated and used for making recommendations. The experimental results demonstrate the usefulness of the proposed algorithm and show its superior performance over the clustering based recommendation algorithms. In addition, the impact of network depth on the algorithm performance is also investigated.},
  language = {en},
  urldate  = {2021-12-02},
  journal  = {Neurocomputing},
  author   = {Zuo, Yi and Zeng, Jiulin and Gong, Maoguo and Jiao, Licheng},
  month    = sep,
  year     = {2016},
  pages    = {51--60}
}

@inproceedings{chen_tgcn_2020,
  author    = {Chen, Bo and Guo, Wei and Tang, Ruiming and Xin, Xin and Ding, Yue and He, Xiuqiang and Wang, Dong},
  title     = {TGCN: Tag Graph Convolutional Network for Tag-Aware Recommendation},
  year      = {2020},
  isbn      = {9781450368599},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3340531.3411927},
  doi       = {10.1145/3340531.3411927},
  abstract  = {Tag-aware recommender systems (TRS) utilize rich tagging records to better depict user portraits and item features. Recently, many efforts have been done to improve TRS with neural networks. However, these solutions rustically rely on the tag-based features for recommendation, which is insufficient to ease the sparsity, ambiguity and redundancy issues introduced by tags, thus hindering the recommendation performance. In this paper, we propose a novel tag-aware recommendation model named Tag Graph Convolutional Network (TGCN), which leverages the contextual semantics of multi-hop neighbors in the user-tag-item graph to alleviate the above issues. Specifically, TGCN first employs type-aware neighbor sampling and aggregation operation to learn the type-specific neighborhood representations. Then we leverage attention mechanism to discriminate the importance of different node types and creatively employ Convolutional Neural Network (CNN) as type-level aggregator to perform vertical and horizontal convolutions for modeling multi-granular feature interactions. Besides, a TransTag regularization function is proposed to accurately identify user's substantive preference. Extensive experiments on three public datasets and a real industrial dataset show that TGCN significantly outperforms state-of-the-art baselines for tag-aware top-N recommendation.},
  booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management},
  pages     = {155-164},
  numpages  = {10},
  keywords  = {representation learning, recommendation, graph neural network, collaborative tagging},
  location  = {Virtual Event, Ireland},
  series    = {CIKM '20}
}

@inproceedings{kipf_gcn_2017,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=SJU4ayYgl},
  timestamp = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{peter_gan_2017,
  author     = {Petar Velickovic and
                Guillem Cucurull and
                Arantxa Casanova and
                Adriana Romero and
                Pietro Li{\`{o}} and
                Yoshua Bengio},
  title      = {Graph Attention Networks},
  journal    = {CoRR},
  year       = {2017},
  url        = {http://arxiv.org/abs/1710.10903},
  eprinttype = {arXiv},
  eprint     = {1710.10903},
  timestamp  = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1710-10903.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{he_deep_2015,
  title    = {Deep {Residual} {Learning} for {Image} {Recognition}},
  url      = {http://arxiv.org/abs/1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate  = {2021-01-19},
  journal  = {arXiv:1512.03385 [cs]},
  author   = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month    = dec,
  year     = {2015},
  note     = {arXiv: 1512.03385
              version: 1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{he_lightgcn_2020,
  author    = {He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, YongDong and Wang, Meng},
  title     = {LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation},
  year      = {2020},
  isbn      = {9781450380164},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3397271.3401063},
  abstract  = {Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.In this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.},
  booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {639–648},
  numpages  = {10}
}

@inproceedings{xu_gin_2018,
  title     = {How Powerful are Graph Neural Networks?},
  author    = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=ryGs6iA5Km}
}

@inproceedings{hamilton_graphsage_2017,
  author    = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Inductive Representation Learning on Large Graphs},
  url       = {https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
  volume    = {30},
  year      = {2017}
}


@inproceedings{zhu_bgnn_2020,
  author    = {Zhu, Hongmin and Feng, Fuli and He, Xiangnan and Wang, Xiang and Li, Yan and Zheng, Kai and Zhang, Yongdong},
  title     = {Bilinear Graph Neural Network with Neighbor Interactions},
  year      = {2021},
  isbn      = {9780999241165},
  abstract  = {Graph Neural Network (GNN) is a powerful model to learn representations and make predictions on graph data. Existing efforts on GNN have largely defined the graph convolution as a weighted sum of the features of the connected nodes to form the representation of the target node. Nevertheless, the operation of weighted sum assumes the neighbor nodes are independent of each other, and ignores the possible interactions between them. When such interactions exist, such as the co-occurrence of two neighbor nodes is a strong signal of the target node's characteristics, existing GNN models may fail to capture the signal. In this work, we argue the importance of modeling the interactions between neighbor nodes in GNN. We propose a new graph convolution operator, which augments the weighted sum with pairwise interactions of the representations of neighbor nodes. We term this framework as Bilinear Graph Neural Network (BGNN), which improves GNN representation ability with bilinear interactions between neighbor nodes. In particular, we specify two BGNN models named BGCN and BGAT, based on the well-known GCN and GAT, respectively. Empirical results on three public benchmarks of semi-supervised node classification verify the effectiveness of BGNN -- BGCN (BGAT) outperforms GCN (GAT) by 1.6% (1.5%) in classification accuracy.},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  articleno = {202},
  numpages  = {7},
  location  = {Yokohama, Yokohama, Japan},
  series    = {IJCAI'20}
}

@inproceedings{wang_neural_2019,
  author    = {Wang, Xiang and He, Xiangnan and Wang, Meng and Feng, Fuli and Chua, Tat-Seng},
  title     = {Neural Graph Collaborative Filtering},
  year      = {2019},
  isbn      = {9781450361729},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3331184.3331267},
  doi       = {10.1145/3331184.3331267},
  abstract  = {Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.},
  booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {165–174},
  numpages  = {10},
  keywords  = {graph neural network, high-order connectivity, collaborative filtering, recommendation, embedding propagation},
  location  = {Paris, France},
  series    = {SIGIR'19}
}

@misc{li_deeper_2018,
  title     = {Deeper {Insights} into {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
  url       = {http://arxiv.org/abs/1801.07606},
  abstract  = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection.},
  language  = {en},
  urldate   = {2022-05-15},
  publisher = {arXiv},
  author    = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
  month     = jan,
  year      = {2018},
  note      = {Number: arXiv:1801.07606
               arXiv:1801.07606 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {Li et al. - 2018 - Deeper Insights into Graph Convolutional Networks .pdf:/home/yin/OneDrive/Zotero/storage/7MR6Z9QK/Li et al. - 2018 - Deeper Insights into Graph Convolutional Networks .pdf:application/pdf}
}

@article{lin_learning_nodate,
  title    = {Learning {Entity} and {Relation} {Embeddings} for {Knowledge} {Graph} {Completion}},
  abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufﬁcient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by ﬁrst projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classiﬁcation and relational fact extraction. Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.},
  language = {en},
  author   = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  pages    = {7},
  file     = {Lin et al. - Learning Entity and Relation Embeddings for Knowle.pdf:/home/yin/OneDrive/Zotero/storage/9K4KWSKS/Lin et al. - Learning Entity and Relation Embeddings for Knowle.pdf:application/pdf}
}

@inproceedings{rendle_bpr_2009,
  author    = {Rendle, Steffen and Balby Marinho, Leandro and Nanopoulos, Alexandros and Schmidt-Thieme, Lars},
  title     = {Learning Optimal Ranking with Tensor Factorization for Tag Recommendation},
  year      = {2009},
  isbn      = {9781605584959},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1557019.1557100},
  doi       = {10.1145/1557019.1557100},
  abstract  = {Tag recommendation is the task of predicting a personalized list of tags for a user given an item. This is important for many websites with tagging capabilities like last.fm or delicious. In this paper, we propose a method for tag recommendation based on tensor factorization (TF). In contrast to other TF methods like higher order singular value decomposition (HOSVD), our method RTF ('ranking with tensor factorization') directly optimizes the factorization model for the best personalized ranking. RTF handles missing values and learns from pairwise ranking constraints. Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems. In all, RTF directly optimizes for the actual problem using a correct interpretation of the data. We provide a gradient descent algorithm to solve our optimization problem. We also provide an improved learning and prediction method with runtime complexity analysis for RTF. The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions. Besides the theoretical analysis, we empirically show that our method outperforms other state-of-the-art tag recommendation methods like FolkRank, PageRank and HOSVD both in quality and prediction runtime.},
  booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {727–736},
  numpages  = {10},
  keywords  = {tensor factorization, tag recommendation, ranking},
  location  = {Paris, France},
  series    = {KDD '09}
}

@misc{kingma_adam_2014,
  doi       = {10.48550/ARXIV.1412.6980},
  url       = {https://arxiv.org/abs/1412.6980},
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{jaiswal_surveyssl_2021,
  author         = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  title          = {A Survey on Contrastive Self-Supervised Learning},
  journal        = {Technologies},
  volume         = {9},
  year           = {2021},
  number         = {1},
  article-number = {2},
  url            = {https://www.mdpi.com/2227-7080/9/1/2},
  issn           = {2227-7080},
  doi            = {10.3390/technologies9010002}
}

@article{liu_sslgoc_2021,
  author  = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {Self-supervised Learning: Generative or Contrastive},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TKDE.2021.3090866}
}

@inproceedings{you_gcl_2020,
  author    = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {5812--5823},
  publisher = {Curran Associates, Inc.},
  title     = {Graph Contrastive Learning with Augmentations},
  url       = {https://proceedings.neurips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{cantador_hetrec_2011,
  author    = {Cantador, Ivan and Brusilovsky, Peter and Kuflik, Tsvi},
  title     = {Second Workshop on Information Heterogeneity and Fusion in Recommender Systems (HetRec2011)},
  year      = {2011},
  isbn      = {9781450306836},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2043932.2044016},
  doi       = {10.1145/2043932.2044016},
  booktitle = {Proceedings of the Fifth ACM Conference on Recommender Systems},
  pages     = {387–388},
  numpages  = {2},
  keywords  = {recommender systems, information heterogeneity, information integration},
  location  = {Chicago, Illinois, USA},
  series    = {RecSys '11}
}

@inproceedings{dacrema_arewe_2019,
  author    = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
  title     = {Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches},
  year      = {2019},
  isbn      = {9781450362436},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3298689.3347058},
  doi       = {10.1145/3298689.3347058},
  abstract  = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models.In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area.},
  booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
  pages     = {101–109},
  numpages  = {9},
  keywords  = {reproducibility, deep learning, recommender systems, evaluation},
  location  = {Copenhagen, Denmark},
  series    = {RecSys '19}
}

@article{zhao_recbole_2021,
  title      = {{RecBole}: {Towards} a {Unified}, {Comprehensive} and {Efficient} {Framework} for {Recommendation} {Algorithms}},
  shorttitle = {{RecBole}},
  url        = {http://arxiv.org/abs/2011.01731},
  abstract   = {In recent years, there are a large number of recommendation algorithms proposed in the literature, from traditional collaborative filtering to deep learning algorithms. However, the concerns about how to standardize open source implementation of recommendation algorithms continually increase in the research community. In the light of this challenge, we propose a unified, comprehensive and efficient recommender system library called RecBole (pronounced as [rEk’boUl@r]), which provides a unified framework to develop and reproduce recommendation algorithms for research purpose. In this library, we implement 73 recommendation models on 28 benchmark datasets, covering the categories of general recommendation, sequential recommendation, context-aware recommendation and knowledge-based recommendation. We implement the RecBole library based on PyTorch, which is one of the most popular deep learning frameworks. Our library is featured in many aspects, including general and extensible data structures, comprehensive benchmark models and datasets, efficient GPU-accelerated execution, and extensive and standard evaluation protocols. We provide a series of auxiliary functions, tools, and scripts to facilitate the use of this library, such as automatic parameter tuning and break-point resume. Such a framework is useful to standardize the implementation and evaluation of recommender systems. The project and documents are released at https://recbole.io/.},
  language   = {en},
  urldate    = {2022-04-17},
  journal    = {arXiv:2011.01731 [cs]},
  author     = {Zhao, Wayne Xin and Mu, Shanlei and Hou, Yupeng and Lin, Zihan and Chen, Yushuo and Pan, Xingyu and Li, Kaiyuan and Lu, Yujie and Wang, Hui and Tian, Changxin and Min, Yingqian and Feng, Zhichao and Fan, Xinyan and Chen, Xu and Wang, Pengfei and Ji, Wendi and Li, Yaliang and Wang, Xiaoling and Wen, Ji-Rong},
  month      = aug,
  year       = {2021},
  note       = {arXiv: 2011.01731},
  keywords   = {Computer Science - Information Retrieval},
  file       = {Zhao et al. - 2021 - RecBole Towards a Unified, Comprehensive and Effi.pdf:/home/yin/OneDrive/Zotero/storage/M4FQV2XI/Zhao et al. - 2021 - RecBole Towards a Unified, Comprehensive and Effi.pdf:application/pdf}
}

@inproceedings{chen_simclr_2020,
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {1597--1607},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url       = {https://proceedings.mlr.press/v119/chen20j.html}
}

@inproceedings{gidaris_unsupervised_2018,
  title     = {Unsupervised Representation Learning by Predicting Image Rotations},
  author    = {Spyros Gidaris and Praveer Singh and Nikos Komodakis},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
  url       = {https://openreview.net/forum?id=S1v4N2l0-}
}


@inproceedings{devlin_bert_2019,
  location   = {Minneapolis, Minnesota},
  title      = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url        = {https://aclanthology.org/N19-1423},
  doi        = {10.18653/v1/N19-1423},
  shorttitle = {{BERT}},
  eventtitle = {{NAACL}-{HLT} 2019},
  pages      = {4171--4186},
  booktitle  = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  publisher  = {Association for Computational Linguistics},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year       = {2019}
}

@inproceedings{gao_simcse_2021,
  title     = {{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings},
  author    = {Gao, Tianyu  and
               Yao, Xingcheng  and
               Chen, Danqi},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = {nov},
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.552},
  doi       = {10.18653/v1/2021.emnlp-main.552},
  pages     = {6894--6910}
}


@inproceedings{wu_self-supervised_2021,
  title     = {Self-supervised {Graph} {Learning} for {Recommendation}},
  url       = {http://arxiv.org/abs/2010.10783},
  doi       = {10.1145/3404835.3462862},
  abstract  = {Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.},
  language  = {en},
  urldate   = {2023-01-26},
  booktitle = {Proceedings of the 44th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  author    = {Wu, Jiancan and Wang, Xiang and Feng, Fuli and He, Xiangnan and Chen, Liang and Lian, Jianxun and Xie, Xing},
  month     = jul,
  year      = {2021},
  note      = {arXiv:2010.10783 [cs]},
  keywords  = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
  pages     = {726--735},
  file      = {Wu et al. - 2021 - Self-supervised Graph Learning for Recommendation.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/DWSP89MQ/Wu et al. - 2021 - Self-supervised Graph Learning for Recommendation.pdf:application/pdf}
}

@inproceedings{yu_simgcl_2022,
  author    = {Yu, Junliang and Yin, Hongzhi and Xia, Xin and Chen, Tong and Cui, Lizhen and Nguyen, Quoc Viet Hung},
  title     = {Are Graph Augmentations Necessary? Simple Graph Contrastive Learning for Recommendation},
  year      = {2022},
  isbn      = {9781450387323},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3477495.3531937},
  doi       = {10.1145/3477495.3531937},
  booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {1294–1303},
  numpages  = {10},
  location  = {Madrid, Spain},
  series    = {SIGIR '22}
}

@misc{oord_rl4cp_2018,
  doi       = {10.48550/ARXIV.1807.03748},
  url       = {https://arxiv.org/abs/1807.03748},
  author    = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Representation Learning with Contrastive Predictive Coding},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{bordes_transe_2013,
  author    = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Translating Embeddings for Modeling Multi-relational Data},
  url       = {https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
  volume    = {26},
  year      = {2013}
}


@inproceedings{krizhevsky_imagenet_2012,
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  volume    = {25},
  url       = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  urldate   = {2023-02-03},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year      = {2012},
  file      = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/TBSEPG8J/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf}
}


@misc{he_neural_2017,
  title     = {Neural {Collaborative} {Filtering}},
  url       = {http://arxiv.org/abs/1708.05031},
  doi       = {10.48550/arXiv.1708.05031},
  abstract  = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.},
  urldate   = {2023-02-03},
  publisher = {arXiv},
  author    = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
  month     = aug,
  year      = {2017},
  note      = {arXiv:1708.05031 [cs]},
  keywords  = {Computer Science - Information Retrieval},
  file      = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/5AWT9VK3/He et al. - 2017 - Neural Collaborative Filtering.pdf:application/pdf}
}


@misc{kipf_semi-supervised_2017,
  title     = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
  url       = {http://arxiv.org/abs/1609.02907},
  doi       = {10.48550/arXiv.1609.02907},
  abstract  = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  urldate   = {2023-02-09},
  publisher = {arXiv},
  author    = {Kipf, Thomas N. and Welling, Max},
  month     = feb,
  year      = {2017},
  note      = {arXiv:1609.02907 [cs, stat]
               version: 4},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning},
  file      = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/REMMAZL6/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf}
}


@incollection{hutchison_information_2006,
  address    = {Berlin, Heidelberg},
  title      = {Information {Retrieval} in {Folksonomies}: {Search} and {Ranking}},
  volume     = {4011},
  isbn       = {978-3-540-34544-2 978-3-540-34545-9},
  shorttitle = {Information {Retrieval} in {Folksonomies}},
  url        = {http://link.springer.com/10.1007/11762256_31},
  abstract   = {Social bookmark tools are rapidly emerging on the Web. In such systems users are setting up lightweight conceptual structures called folksonomies. The reason for their immediate success is the fact that no speciﬁc skills are needed for participating. At the moment, however, the information retrieval support is limited. We present a formal model and a new search algorithm for folksonomies, called FolkRank, that exploits the structure of the folksonomy. The proposed algorithm is also applied to ﬁnd communities within the folksonomy and is used to structure search results. All ﬁndings are demonstrated on a large scale dataset.},
  language   = {en},
  urldate    = {2021-11-30},
  booktitle  = {The {Semantic} {Web}: {Research} and {Applications}},
  publisher  = {Springer Berlin Heidelberg},
  author     = {Hotho, Andreas and Jäschke, Robert and Schmitz, Christoph and Stumme, Gerd},
  editor     = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Sure, York and Domingue, John},
  year       = {2006},
  doi        = {10.1007/11762256_31},
  note       = {Series Title: Lecture Notes in Computer Science},
  pages      = {411--426},
  file       = {Hotho 等。 - 2006 - Information Retrieval in Folksonomies Search and .pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/W2EXWYDW/Hotho 等。 - 2006 - Information Retrieval in Folksonomies Search and .pdf:application/pdf}
}

@inproceedings{zhen_tagicofi_2009,
  author    = {Zhen, Yi and Li, Wu-Jun and Yeung, Dit-Yan},
  title     = {TagiCoFi: Tag Informed Collaborative Filtering},
  year      = {2009},
  isbn      = {9781605584355},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1639714.1639727},
  doi       = {10.1145/1639714.1639727},
  abstract  = {Besides the rating information, an increasing number of modern recommender systems also allow the users to add personalized tags to the items. Such tagging information may provide very useful information for item recommendation, because the users' interests in items can be implicitly reflected by the tags that they often use. Although some content-based recommender systems have made preliminary attempts recently to utilize tagging information to improve the recommendation performance, few recommender systems based on collaborative filtering (CF) have employed tagging information to help the item recommendation procedure. In this paper, we propose a novel framework, called tag informed collaborative filtering (TagiCoFi), to seamlessly integrate tagging information into the CF procedure. Experimental results demonstrate that TagiCoFi outperforms its counterpart which discards the tagging information even when it is available, and achieves state-of-the-art performance.},
  booktitle = {Proceedings of the Third ACM Conference on Recommender Systems},
  pages     = {69–76},
  numpages  = {8},
  keywords  = {recommender systems, tag, collaborative filtering},
  location  = {New York, New York, USA},
  series    = {RecSys '09}
}

@inproceedings{shepitsen_personalized_2008,
  author    = {Shepitsen, Andriy and Gemmell, Jonathan and Mobasher, Bamshad and Burke, Robin},
  title     = {Personalized Recommendation in Social Tagging Systems Using Hierarchical Clustering},
  year      = {2008},
  isbn      = {9781605580937},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1454008.1454048},
  doi       = {10.1145/1454008.1454048},
  abstract  = {Collaborative tagging applications allow Internet users to annotate resources with personalized tags. The complex network created by many annotations, often called a folksonomy, permits users the freedom to explore tags, resources or even other user's profiles unbound from a rigid predefined conceptual hierarchy. However, the freedom afforded users comes at a cost: an uncontrolled vocabulary can result in tag redundancy and ambiguity hindering navigation. Data mining techniques, such as clustering, provide a means to remedy these problems by identifying trends and reducing noise. Tag clusters can also be used as the basis for effective personalized recommendation assisting users in navigation. We present a personalization algorithm for recommendation in folksonomies which relies on hierarchical tag clusters. Our basic recommendation framework is independent of the clustering method, but we use a context-dependent variant of hierarchical agglomerative clustering which takes into account the user's current navigation context in cluster selection. We present extensive experimental results on two real world dataset. While the personalization algorithm is successful in both cases, our results suggest that folksonomies encompassing only one topic domain, rather than many topics, present an easier target for recommendation, perhaps because they are more focused and often less sparse. Furthermore, context dependent cluster selection, an integral step in our personalization algorithm, demonstrates more utility for recommendation in multi-topic folksonomies than in single-topic folksonomies. This observation suggests that topic selection is an important strategy for recommendation in multi-topic folksonomies.},
  booktitle = {Proceedings of the 2008 ACM Conference on Recommender Systems},
  pages     = {259–266},
  numpages  = {8},
  keywords  = {personalization, clustering, recommender systems, collaborative tagging},
  location  = {Lausanne, Switzerland},
  series    = {RecSys '08}
}

@inproceedings{peng_collaborative_2010,
  author    = {Peng, Jing and Zeng, Daniel Dajun and Zhao, Huimin and Wang, Fei-yue},
  title     = {Collaborative Filtering in Social Tagging Systems Based on Joint Item-Tag Recommendations},
  year      = {2010},
  isbn      = {9781450300995},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1871437.1871541},
  doi       = {10.1145/1871437.1871541},
  abstract  = {Tapping into the wisdom of the crowd, social tagging can be considered an alternative mechanism - as opposed to Web search - for organizing and discovering information on the Web. Effective tag-based recommendation of information items, such as Web resources, is a critical aspect of this social information discovery mechanism. A precise understanding of the information structure of social tagging systems lies at the core of an effective tag-based recommendation method. While most of the existing research either implicitly or explicitly assumes a simple tripartite graph structure for this purpose, we propose a comprehensive information structure to capture all types of co-occurrence information in the tagging data. Based on the proposed information structure, we further propose a unified user profiling scheme to make full use of all available information. Finally, supported by our proposed user profile, we propose a novel framework for collaborative filtering in social tagging systems. In our proposed framework, we first generate joint item-tag recommendations, with tags indicating topical interests of users in target items. These joint recommendations are then refined by the wisdom from the crowd and projected to the item space for final item recommendations. Evaluation using three real-world datasets shows that our proposed recommendation approach significantly outperformed state-of-the-art approaches.},
  booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
  pages     = {809–818},
  numpages  = {10},
  keywords  = {collaborative filtering, explanation, social tagging, joint recommendation, tagging structure},
  location  = {Toronto, ON, Canada},
  series    = {CIKM '10}
}

@article{zhang_personalized_2010,
  title    = {Personalized recommendation via integrated diffusion on user–item–tag tripartite graphs},
  volume   = {389},
  issn     = {0378-4371},
  url      = {https://www.sciencedirect.com/science/article/pii/S0378437109006839},
  doi      = {https://doi.org/10.1016/j.physa.2009.08.036},
  abstract = {Personalized recommender systems are confronting great challenges of accuracy, diversification and novelty, especially when the data set is sparse and lacks accessorial information, such as user profiles, item attributes and explicit ratings. Collaborative tags contain rich information about personalized preferences and item contents, and are therefore potential to help in providing better recommendations. In this article, we propose a recommendation algorithm based on an integrated diffusion on user–item–tag tripartite graphs. We use three benchmark data sets, Del.icio.us, MovieLens and BibSonomy, to evaluate our algorithm. Experimental results demonstrate that the usage of tag information can significantly improve accuracy, diversification and novelty of recommendations.},
  number   = {1},
  journal  = {Physica A: Statistical Mechanics and its Applications},
  author   = {Zhang, Zi-Ke and Zhou, Tao and Zhang, Yi-Cheng},
  year     = {2010},
  keywords = {Complex networks, Diffusion, Folksonomy, Infophysics, Personalized recommendation},
  pages    = {179--186}
}

@article{zhang_solving_2012,
  title    = {Solving the {Cold}-{Start} {Problem} in {Recommender} {Systems} with {Social} {Tags}},
  volume   = {39},
  issn     = {09574174},
  url      = {http://arxiv.org/abs/1004.3732},
  doi      = {10.1016/j.eswa.2012.03.025},
  abstract = {In this paper, based on the user-tag-object tripartite graphs, we propose a recommendation algorithm, which considers social tags as an important role for information retrieval. Besides its low cost of computational time, the experiment results of two real-world data sets, {\textbackslash}emph\{Del.icio.us\} and {\textbackslash}emph\{MovieLens\}, show it can enhance the algorithmic accuracy and diversity. Especially, it can obtain more personalized recommendation results when users have diverse topics of tags. In addition, the numerical results on the dependence of algorithmic accuracy indicates that the proposed algorithm is particularly effective for small degree objects, which reminds us of the well-known {\textbackslash}emph\{cold-start\} problem in recommender systems. Further empirical study shows that the proposed algorithm can significantly solve this problem in social tagging systems with heterogeneous object degree distributions.},
  number   = {12},
  urldate  = {2021-12-05},
  journal  = {Expert Systems with Applications},
  author   = {Zhang, Zi-Ke and Liu, Chuang and Zhang, Yi-Cheng and Zhou, Tao},
  month    = sep,
  year     = {2012},
  note     = {arXiv: 1004.3732},
  keywords = {Computer Science - Information Retrieval, Physics - Physics and Society},
  pages    = {10990--11000}
}

@article{zhao_folkrank_2021,
  title      = {{FolkRank}++: {An} {Optimization} of {FolkRank} {Tag} {Recommendation} {Algorithm} {Integrating} {User} and {Item} {Information}},
  volume     = {15},
  issn       = {19767277},
  shorttitle = {{FolkRank}++},
  url        = {http://itiis.org/digital-library/24227},
  doi        = {10.3837/tiis.2021.01.001},
  abstract   = {The graph-based tag recommendation algorithm FolkRank can effectively utilize the relationships between three entities, namely users, items and tags, and achieve better tag recommendation performance. However, FolkRank does not consider the internal relationships of user-user, item-item and tag-tag. This leads to the failure of FolkRank to effectively map the tagging behavior which contains user neighbors and item neighbors to a tripartite graph. For item-item relationships, we can dig out items that are very similar to the target item, even though the target item may not have a strong connection to these similar items in the user-item-tag graph of FolkRank. Hence this paper proposes an improved FolkRank algorithm named FolkRank++, which fully considers the user-user and item-item internal relationships in tag recommendation by adding the correlation information between users or items. Based on the traditional FolkRank algorithm, an initial weight is also given to target user and target item's neighbors to supply the user-user and item-item relationships. The above work is mainly completed from two aspects: (1) Finding items similar to target item according to the attribute information, and obtaining similar users of the target user according to the history behavior of the user tagging items. (2) Calculating the weighted degree of items and users to evaluate their importance, then assigning initial weights to similar items and users. Experimental results show that this method has better recommendation performance.},
  language   = {en},
  number     = {1},
  urldate    = {2021-12-05},
  journal    = {KSII Transactions on Internet and Information Systems},
  month      = jan,
  year       = {2021}
}

@inproceedings{rendle_learning_2009,
  address   = {Paris, France},
  title     = {Learning optimal ranking with tensor factorization for tag recommendation},
  isbn      = {978-1-60558-495-9},
  url       = {http://portal.acm.org/citation.cfm?doid=1557019.1557100},
  doi       = {10.1145/1557019.1557100},
  abstract  = {Tag recommendation is the task of predicting a personalized list of tags for a user given an item. This is important for many websites with tagging capabilities like last.fm or delicious. In this paper, we propose a method for tag recommendation based on tensor factorization (TF). In contrast to other TF methods like higher order singular value decomposition (HOSVD), our method RTF (‘ranking with tensor factorization’) directly optimizes the factorization model for the best personalized ranking. RTF handles missing values and learns from pairwise ranking constraints. Our optimization criterion for TF is motivated by a detailed analysis of the problem and of interpretation schemes for the observed data in tagging systems. In all, RTF directly optimizes for the actual problem using a correct interpretation of the data. We provide a gradient descent algorithm to solve our optimization problem. We also provide an improved learning and prediction method with runtime complexity analysis for RTF. The prediction runtime of RTF is independent of the number of observations and only depends on the factorization dimensions. Besides the theoretical analysis, we empirically show that our method outperforms other state-of-theart tag recommendation methods like FolkRank, PageRank and HOSVD both in quality and prediction runtime.},
  language  = {en},
  urldate   = {2021-11-28},
  booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '09},
  publisher = {ACM Press},
  author    = {Rendle, Steffen and Balby Marinho, Leandro and Nanopoulos, Alexandros and Schmidt-Thieme, Lars},
  year      = {2009},
  pages     = {727}
}

@article{li_tag-aware_2019,
  title    = {Tag-aware recommendation based on {Bayesian} personalized ranking and feature mapping},
  volume   = {23},
  issn     = {1088467X, 15714128},
  url      = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/IDA-193982},
  doi      = {10.3233/IDA-193982},
  abstract = {Collaborative ﬁltering recommendation with implicit feedbacks (i.e., clicks, views, check-ins) has been gaining increasing attention in various real applications. Tagging information is the common resource to complement implicit feedbacks to assist collaborative ﬁltering recommendation. However, existing tag-aware recommendation methods still suffer from the problem of high dimension and sparsity of tagging information. They also fail to realize that recommendation is inherent a ranking-oriented optimization task. To this end, we propose a novel tag-aware recommendation framework by incorporating tag mapping scheme into ranking-based collaborative ﬁltering model, to boost ranking-oriented personalized recommendation performance. We ﬁrst build ranking-oriented optimization model based on Bayesian personalized ranking optimization criterion with matrix factorization, by leveraging implicit feedbacks to learn the latent feature vectors of users and items. Then, we propose an explicit-to-implicit feature mapping scheme, mapping the high-dimensional and sparse explicit tags (i.e., user-tag weighting matrix and item-tag weighting matrix) to low-dimensional and compact implicit features of uses and items. This could serve as the regularization constraint of latent features derived from implicit feedbacks. To further enhance recommendation performance, we also introduce users’ neighbor relationships to regularize user latent features based on manifold learning. Experiments on real-world recommendation datasets show that the proposed recommendation method outperformed competing methods on ranking-oriented recommendation performance.},
  language = {en},
  number   = {3},
  urldate  = {2021-11-25},
  journal  = {Intelligent Data Analysis},
  author   = {Li, Hongmei and Diao, Xingchun and Cao, Jianjun and Zhang, Lei and Feng, Qin},
  month    = apr,
  year     = {2019},
  pages    = {641--659}
}


@misc{zhang_deep_2016,
  title      = {Deep {Learning} over {Multi}-field {Categorical} {Data}: {A} {Case} {Study} on {User} {Response} {Prediction}},
  shorttitle = {Deep {Learning} over {Multi}-field {Categorical} {Data}},
  url        = {http://arxiv.org/abs/1601.02376},
  doi        = {10.48550/arXiv.1601.02376},
  abstract   = {Predicting user responses, such as click-through rate and conversion rate, are critical in many web applications including web search, personalised recommendation, and online advertising. Different from continuous raw features that we usually found in the image and audio domains, the input features in web space are always of multi-field and are mostly discrete and categorical while their dependencies are little known. Major user response prediction models have to either limit themselves to linear models or require manually building up high-order combination features. The former loses the ability of exploring feature interactions, while the latter results in a heavy computation in the large feature space. To tackle the issue, we propose two novel models using deep neural networks (DNNs) to automatically learn effective patterns from categorical feature interactions and make predictions of users' ad clicks. To get our DNNs efficiently work, we propose to leverage three feature transformation methods, i.e., factorisation machines (FMs), restricted Boltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper presents the structure of our models and their efficient training algorithms. The large-scale experiments with real-world data demonstrate that our methods work better than major state-of-the-art models.},
  urldate    = {2023-02-09},
  publisher  = {arXiv},
  author     = {Zhang, Weinan and Du, Tianming and Wang, Jun},
  month      = jan,
  year       = {2016},
  note       = {arXiv:1601.02376 [cs]},
  keywords   = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/ZDAG869I/Zhang et al. - 2016 - Deep Learning over Multi-field Categorical Data A.pdf:application/pdf}
}

@misc{qu_product-based_2018,
  title     = {Product-based {Neural} {Networks} for {User} {Response} {Prediction} over {Multi}-field {Categorical} {Data}},
  url       = {http://arxiv.org/abs/1807.00311},
  doi       = {10.48550/arXiv.1807.00311},
  abstract  = {User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this paper, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network (PNN) which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network In Network (PIN) which can generalize previous models. Extensive experiments on 4 industrial datasets and 1 contest dataset demonstrate that our models consistently outperform 8 baselines on both AUC and log loss. Besides, PIN makes great CTR improvement (relatively 34.67\%) in online A/B test.},
  urldate   = {2023-02-09},
  publisher = {arXiv},
  author    = {Qu, Yanru and Fang, Bohui and Zhang, Weinan and Tang, Ruiming and Niu, Minzhe and Guo, Huifeng and Yu, Yong and He, Xiuqiang},
  month     = jul,
  year      = {2018},
  note      = {arXiv:1807.00311 [cs, stat]},
  keywords  = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{shan_deep_2016,
  address    = {San Francisco California USA},
  title      = {Deep {Crossing}: {Web}-{Scale} {Modeling} without {Manually} {Crafted} {Combinatorial} {Features}},
  isbn       = {978-1-4503-4232-2},
  shorttitle = {Deep {Crossing}},
  url        = {https://dl.acm.org/doi/10.1145/2939672.2939704},
  doi        = {10.1145/2939672.2939704},
  abstract   = {Manually crafted combinatorial features have been the “secret sauce” behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units.},
  language   = {en},
  urldate    = {2023-02-09},
  booktitle  = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher  = {ACM},
  author     = {Shan, Ying and Hoens, T. Ryan and Jiao, Jian and Wang, Haijing and Yu, Dong and Mao, Jc},
  month      = aug,
  year       = {2016},
  pages      = {255--262},
  file       = {Shan et al. - 2016 - Deep Crossing Web-Scale Modeling without Manually.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/LUYKI2R2/Shan et al. - 2016 - Deep Crossing Web-Scale Modeling without Manually.pdf:application/pdf}
}

@inproceedings{sedhain_autorec_2015,
  address    = {Florence Italy},
  title      = {{AutoRec}: {Autoencoders} {Meet} {Collaborative} {Filtering}},
  isbn       = {978-1-4503-3473-0},
  shorttitle = {{AutoRec}},
  url        = {https://dl.acm.org/doi/10.1145/2740908.2742726},
  doi        = {10.1145/2740908.2742726},
  abstract   = {This paper proposes AutoRec, a novel autoencoder framework for collaborative ﬁltering (CF). Empirically, AutoRec’s compact and eﬃciently trainable model outperforms stateof-the-art CF techniques (biased matrix factorization, RBMCF and LLORMA) on the Movielens and Netﬂix datasets.},
  language   = {en},
  urldate    = {2023-02-09},
  booktitle  = {Proceedings of the 24th {International} {Conference} on {World} {Wide} {Web}},
  publisher  = {ACM},
  author     = {Sedhain, Suvash and Menon, Aditya Krishna and Sanner, Scott and Xie, Lexing},
  month      = may,
  year       = {2015},
  pages      = {111--112},
  file       = {Sedhain et al. - 2015 - AutoRec Autoencoders Meet Collaborative Filtering.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/TJ4HIXRQ/Sedhain et al. - 2015 - AutoRec Autoencoders Meet Collaborative Filtering.pdf:application/pdf}
}

@misc{noauthor_180205814_nodate,
  title   = {[1802.05814] {Variational} {Autoencoders} for {Collaborative} {Filtering}},
  url     = {https://arxiv.org/abs/1802.05814},
  urldate = {2023-02-09}
}

@inproceedings{liang_variational_2018,
  address   = {Republic and Canton of Geneva, CHE},
  series    = {{WWW} '18},
  title     = {Variational {Autoencoders} for {Collaborative} {Filtering}},
  isbn      = {978-1-4503-5639-8},
  url       = {https://doi.org/10.1145/3178876.3186150},
  doi       = {10.1145/3178876.3186150},
  abstract  = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  urldate   = {2023-02-09},
  booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference}},
  publisher = {International World Wide Web Conferences Steering Committee},
  author    = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  month     = apr,
  year      = {2018},
  keywords  = {bayesian models, collaborative filtering, implicit feedback, recommender systems, variational autoencoder},
  pages     = {689--698},
  file      = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/QJ87SV2M/Liang et al. - 2018 - Variational Autoencoders for Collaborative Filteri.pdf:application/pdf}
}

@article{he_nais_2018,
  title      = {{NAIS}: {Neural} {Attentive} {Item} {Similarity} {Model} for {Recommendation}},
  volume     = {30},
  issn       = {1041-4347, 1558-2191, 2326-3865},
  shorttitle = {{NAIS}},
  url        = {http://arxiv.org/abs/1809.07053},
  doi        = {10.1109/TKDE.2018.2831682},
  abstract   = {Item-to-item collaborative filtering (aka. item-based CF) has been long used for building recommender systems in industrial settings, owing to its interpretability and efficiency in real-time personalization. It builds a user's profile as her historically interacted items, recommending new items that are similar to the user's profile. As such, the key to an item-based CF method is in the estimation of item similarities. Early approaches use statistical measures such as cosine similarity and Pearson coefficient to estimate item similarities, which are less accurate since they lack tailored optimization for the recommendation task. In recent years, several works attempt to learn item similarities from data, by expressing the similarity as an underlying model and estimating model parameters by optimizing a recommendation-aware objective function. While extensive efforts have been made to use shallow linear models for learning item similarities, there has been relatively less work exploring nonlinear neural network models for item-based CF. In this work, we propose a neural network model named Neural Attentive Item Similarity model (NAIS) for item-based CF. The key to our design of NAIS is an attention network, which is capable of distinguishing which historical items in a user profile are more important for a prediction. Compared to the state-of-the-art item-based CF method Factored Item Similarity Model (FISM), our NAIS has stronger representation power with only a few additional parameters brought by the attention network. Extensive experiments on two public benchmarks demonstrate the effectiveness of NAIS. This work is the first attempt that designs neural network models for item-based CF, opening up new research possibilities for future developments of neural recommender systems.},
  number     = {12},
  urldate    = {2023-02-09},
  journal    = {IEEE Transactions on Knowledge and Data Engineering},
  author     = {He, Xiangnan and He, Zhankui and Song, Jingkuan and Liu, Zhenguang and Jiang, Yu-Gang and Chua, Tat-Seng},
  month      = dec,
  year       = {2018},
  note       = {arXiv:1809.07053 [cs]},
  keywords   = {Computer Science - Information Retrieval},
  pages      = {2354--2366}
}

@inproceedings{chen_attentive_2017,
  address    = {New York, NY, USA},
  series     = {{SIGIR} '17},
  title      = {Attentive {Collaborative} {Filtering}: {Multimedia} {Recommendation} with {Item}- and {Component}-{Level} {Attention}},
  isbn       = {978-1-4503-5022-8},
  shorttitle = {Attentive {Collaborative} {Filtering}},
  url        = {https://doi.org/10.1145/3077136.3080797},
  doi        = {10.1145/3077136.3080797},
  abstract   = {Multimedia content is dominating today's Web information. The nature of multimedia user-item interactions is 1/0 binary implicit feedback (e.g., photo likes, video views, song downloads, etc.), which can be collected at a larger scale with a much lower cost than explicit feedback (e.g., product ratings). However, the majority of existing collaborative filtering (CF) systems are not well-designed for multimedia recommendation, since they ignore the implicitness in users' interactions with multimedia content. We argue that, in multimedia recommendation, there exists item- and component-level implicitness which blurs the underlying users' preferences. The item-level implicitness means that users' preferences on items (e.g. photos, videos, songs, etc.) are unknown, while the component-level implicitness means that inside each item users' preferences on different components (e.g. regions in an image, frames of a video, etc.) are unknown. For example, a 'view'' on a video does not provide any specific information about how the user likes the video (i.e.item-level) and which parts of the video the user is interested in (i.e.component-level). In this paper, we introduce a novel attention mechanism in CF to address the challenging item- and component-level implicit feedback in multimedia recommendation, dubbed Attentive Collaborative Filtering (ACF). Specifically, our attention model is a neural network that consists of two attention modules: the component-level attention module, starting from any content feature extraction network (e.g. CNN for images/videos), which learns to select informative components of multimedia items, and the item-level attention module, which learns to score the item preferences. ACF can be seamlessly incorporated into classic CF models with implicit feedback, such as BPR and SVD++, and efficiently trained using SGD. Through extensive experiments on two real-world multimedia Web services: Vine and Pinterest, we show that ACF significantly outperforms state-of-the-art CF methods.},
  urldate    = {2023-02-09},
  booktitle  = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  publisher  = {Association for Computing Machinery},
  author     = {Chen, Jingyuan and Zhang, Hanwang and He, Xiangnan and Nie, Liqiang and Liu, Wei and Chua, Tat-Seng},
  month      = aug,
  year       = {2017},
  keywords   = {attention, collaborative filtering, implicit feedback, multimedia recommendation},
  pages      = {335--344}
}

@article{xu_tag-aware_2016,
  title    = {Tag-{Aware} {Personalized} {Recommendation} {Using} a {Deep}-{Semantic} {Similarity} {Model} with {Negative} {Sampling}},
  url      = {https://dl.acm.org/doi/10.1145/2983323.2983874},
  doi      = {10.1145/2983323.2983874},
  abstract = {With the rapid growth of social tagging systems, many efforts have been put on tag-aware personalized recommendation. However, due to uncontrolled vocabularies, social tags are usually redundant, sparse, and ambiguous. In this paper, we propose a deep neural network approach to solve this problem by mapping both the tag-based user and item profiles to an abstract deep feature space, where the deep-semantic similarities between users and their target items (resp., irrelevant items) are maximized (resp., minimized). Due to huge numbers of online items, the training of this model is usually computationally expensive in the real-world context. Therefore, we introduce negative sampling, which significantly increases the model's training efficiency (109.6 times quicker) and ensures the scalability in practice. Experimental results show that our model can significantly outperform the state-of-the-art baselines in tag-aware personalized recommendation: e.g., its mean reciprocal rank is between 5.7 and 16.5 times better than the baselines.},
  language = {en},
  urldate  = {2023-02-09},
  journal  = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
  author   = {Xu, Zhenghua and Chen, Cheng and Lukasiewicz, Thomas and Miao, Yishu and Meng, Xiangwu},
  month    = oct,
  year     = {2016},
  note     = {Conference Name: CIKM'16: ACM Conference on Information and Knowledge Management
              ISBN: 9781450340731
              Place: Indianapolis Indiana USA
              Publisher: ACM},
  pages    = {1921--1924},
  file     = {Submitted Version:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/C2CBD4VN/Xu et al. - 2016 - Tag-Aware Personalized Recommendation Using a Deep.pdf:application/pdf}
}

@inproceedings{huang_tag-aware_2021,
  title     = {Tag-aware {Attentional} {Graph} {Neural} {Networks} for {Personalized} {Tag} {Recommendation}},
  doi       = {10.1109/IJCNN52387.2021.9533380},
  abstract  = {Personalized tag recommender systems recommend a series of tags for items by leveraging users' historical records, which helps tag-aware recommender systems (TRS) to better depict user profiles and item characteristics. However, existing personalized tag recommendation solutions are insufficient to capture the collaborative signal hidden in the interactions among entities without considering reasonable correlations, since neighborhood messages are treated as the same weights when constructing graph-structured data, resulting in decreased accuracy in making recommendations. In this paper, we propose a Tag-aware Attentional Graph Neural Network (TA-GNN), which integrates the attention mechanism into tag-based graph neural networks to alleviate the above issues. Specifically, we extract the user-tag interaction and the item-tag interaction from the user-tag-item graph structure. For each interaction, we exploit the contextual semantics of multi-hop neighbors by leveraging attentional strategy on graph neural networks to discriminate the importance of different connected nodes. In this way, we effectively extract collaborative signals of neighborhood representations and capture the potential information in an explicit manner. Extensive experiments on three public datasets show that our proposed TA-GNN outperforms the state-of-the-art personalized tag recommendation baselines.},
  booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
  author    = {Huang, Ruoran and Han, Chuanqi and Cui, Li},
  month     = jul,
  year      = {2021},
  note      = {ISSN: 2161-4407},
  keywords  = {Collaboration, Correlation, Data mining, Graph neural networks, Recommender systems, Semantics, Spread spectrum communication},
  pages     = {1--8}
}

@incollection{prechelt_early_2012,
  address   = {Berlin, Heidelberg},
  series    = {Lecture {Notes} in {Computer} {Science}},
  title     = {Early {Stopping} — {But} {When}?},
  isbn      = {978-3-642-35289-8},
  url       = {https://doi.org/10.1007/978-3-642-35289-8_5},
  abstract  = {Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (“early stopping”). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4\% on average), but cost much more training time (here: about factor 4 longer on average).},
  language  = {en},
  urldate   = {2023-02-13},
  booktitle = {Neural {Networks}: {Tricks} of the {Trade}: {Second} {Edition}},
  publisher = {Springer},
  author    = {Prechelt, Lutz},
  editor    = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  year      = {2012},
  doi       = {10.1007/978-3-642-35289-8_5},
  keywords  = {Generalization Error, Neural Information Processing System, Training Algorithm, Training Time, Validation Error},
  pages     = {53--67}
}

@article{glorot_understanding_nodate,
  title    = {Understanding the difficulty of training deep feedforward neural networks},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully
              trained, since then several algorithms have been
              shown to successfully train them, with experimental results showing the superiority of deeper
              vs less deep architectures. All these experimental results were obtained with new initialization
              or training mechanisms. Our objective here is to
              understand better why standard gradient descent
              from random initialization is doing so poorly
              with deep neural networks, to better understand
              these recent relative successes and help design
              better algorithms in the future. We first observe
              the influence of the non-linear activations functions. We find that the logistic sigmoid activation
              is unsuited for deep networks with random initialization because of its mean value, which can
              drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units
              can move out of saturation by themselves, albeit
              slowly, and explaining the plateaus sometimes
              seen when training neural networks. We find that
              a new non-linearity that saturates less can often
              be beneficial. Finally, we study how activations
              and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian
              associated with each layer are far from 1. Based
              on these considerations, we propose a new initialization scheme that brings substantially faster
              convergence.},
  language = {en},
  author   = {Glorot, Xavier and Bengio, Yoshua},
  file     = {Understanding the difficulty of training deep feedforward neural networks}
}

@book{roelleke_information_2022,
	series = {Synthesis {Lectures} on {Information} {Concepts}, {Retrieval}, and {Services}},
	title = {Information {Retrieval} {Models}: {Foundations} \& {Relationships}},
	isbn = {978-3-031-02328-6},
	url = {https://books.google.com.hk/books?id=YX9yEAAAQBAJ},
	publisher = {Springer International Publishing},
	author = {Roelleke, T.},
	year = {2022},
}

@misc{wang_theoretical_2013,
	title = {A {Theoretical} {Analysis} of {NDCG} {Type} {Ranking} {Measures}},
	url = {http://arxiv.org/abs/1304.6480},
	abstract = {A central problem in ranking is to design a ranking measure for evaluation of ranking functions. In this paper we study, from a theoretical perspective, the widely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures. Although there are extensive empirical studies of NDCG, little is known about its theoretical properties. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result is very surprising. It seems to imply that NDCG cannot differentiate good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. In order to have a deeper understanding of ranking measures in general, we propose a notion referred to as consistent distinguishability. This notion captures the intuition that a ranking measure should have such a property: For every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets. We show that NDCG with logarithmic discount has consistent distinguishability although it converges to the same limit for all ranking functions. We next characterize the set of all feasible discount functions for NDCG according to the concept of consistent distinguishability. Specifically we show that whether NDCG has consistent distinguishability depends on how fast the discount decays, and 1/r is a critical point. We then turn to the cut-off version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for various choices of k and the discount functions. Experimental results on real Web search datasets agree well with the theory.},
	urldate = {2023-02-16},
	publisher = {arXiv},
	author = {Wang, Yining and Wang, Liwei and Li, Yuanzhi and He, Di and Liu, Tie-Yan and Chen, Wei},
	month = apr,
	year = {2013},
	note = {arXiv:1304.6480 [cs, stat]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/FAC9L7CZ/Wang et al. - 2013 - A Theoretical Analysis of NDCG Type Ranking Measur.pdf:application/pdf},
}


@incollection{craswell_mean_2009,
	address = {Boston, MA},
	title = {Mean {Reciprocal} {Rank}},
	isbn = {978-0-387-39940-9},
	url = {https://doi.org/10.1007/978-0-387-39940-9_488},
	language = {en},
	urldate = {2023-02-16},
	booktitle = {Encyclopedia of {Database} {Systems}},
	publisher = {Springer US},
	author = {Craswell, Nick},
	editor = {LIU, LING and ÖZSU, M. TAMER},
	year = {2009},
	doi = {10.1007/978-0-387-39940-9_488},
	pages = {1703--1703},
	file = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/Y6UMIA9C/Craswell - 2009 - Mean Reciprocal Rank.pdf:application/pdf},
}

@misc{mao_simplex_2021,
	title = {{SimpleX}: {A} {Simple} and {Strong} {Baseline} for {Collaborative} {Filtering}},
	shorttitle = {{SimpleX}},
	url = {http://arxiv.org/abs/2109.12613},
	doi = {10.48550/arXiv.2109.12613},
	abstract = {Collaborative filtering (CF) is a widely studied research topic in recommender systems. The learning of a CF model generally depends on three major components, namely interaction encoder, loss function, and negative sampling. While many existing studies focus on the design of more powerful interaction encoders, the impacts of loss functions and negative sampling ratios have not yet been well explored. In this work, we show that the choice of loss function as well as negative sampling ratio is equivalently important. More specifically, we propose the cosine contrastive loss (CCL) and further incorporate it to a simple unified CF model, dubbed SimpleX. Extensive experiments have been conducted on 11 benchmark datasets and compared with 29 existing CF models in total. Surprisingly, the results show that, under our CCL loss and a large negative sampling ratio, SimpleX can surpass most sophisticated state-of-the-art models by a large margin (e.g., max 48.5\% improvement in NDCG@20 over LightGCN). We believe that SimpleX could not only serve as a simple strong baseline to foster future research on CF, but also shed light on the potential research direction towards improving loss function and negative sampling.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Mao, Kelong and Zhu, Jieming and Wang, Jinpeng and Dai, Quanyu and Dong, Zhenhua and Xiao, Xi and He, Xiuqiang},
	month = sep,
	year = {2021},
	note = {arXiv:2109.12613 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/2RKBB9EK/Mao et al. - 2021 - SimpleX A Simple and Strong Baseline for Collabor.pdf:application/pdf},
}

@misc{tang_multi-sample_2021,
	title = {Multi-{Sample} based {Contrastive} {Loss} for {Top}-k {Recommendation}},
	url = {http://arxiv.org/abs/2109.00217},
	doi = {10.48550/arXiv.2109.00217},
	abstract = {The top-k recommendation is a fundamental task in recommendation systems which is generally learned by comparing positive and negative pairs. The Contrastive Loss (CL) is the key in contrastive learning that has received more attention recently and we find it is well suited for top-k recommendations. However, it is a problem that CL treats the importance of the positive and negative samples as the same. On the one hand, CL faces the imbalance problem of one positive sample and many negative samples. On the other hand, positive items are so few in sparser datasets that their importance should be emphasized. Moreover, the other important issue is that the sparse positive items are still not sufficiently utilized in recommendations. So we propose a new data augmentation method by using multiple positive items (or samples) simultaneously with the CL loss function. Therefore, we propose a Multi-Sample based Contrastive Loss (MSCL) function which solves the two problems by balancing the importance of positive and negative samples and data augmentation. And based on the graph convolution network (GCN) method, experimental results demonstrate the state-of-the-art performance of MSCL. The proposed MSCL is simple and can be applied in many methods. We will release our code on GitHub upon the acceptance.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Tang, Hao and Zhao, Guoshuai and Wu, Yuxia and Qian, Xueming},
	month = sep,
	year = {2021},
	note = {arXiv:2109.00217 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@article{benz_bibsonomy_2010,
  address = {Berlin/Heidelberg},
  author = {Benz, Dominik and Hotho, Andreas and Jäschke, Robert and Krause, Beate and Mitzlaff, Folke and Schmitz, Christoph and Stumme, Gerd},
  biburl = {https://www.bibsonomy.org/bibtex/2c9437d5ec56ba949f533aeec00f571e3/jaeschke},
  doi = {10.1007/s00778-010-0208-4},
  issn = {1066-8888},
  journal = {The VLDB Journal},
  month = dec,
  number = {6},
  pages = {849--875},
  publisher = {Springer},
  title = {The Social Bookmark and Publication Management System {BibSonomy}},
  url = {http://www.kde.cs.uni-kassel.de/pub/pdf/benz2010social.pdf},
  volume = {19},
  year = {2010}
}

@inproceedings{bischoff_can_2008,
	address = {Napa Valley, California, USA},
	title = {Can all tags be used for search?},
	isbn = {978-1-59593-991-3},
	url = {http://portal.acm.org/citation.cfm?doid=1458082.1458112},
	doi = {10.1145/1458082.1458112},
	abstract = {Collaborative tagging has become an increasingly popular means for sharing and organizing Web resources, leading to a huge amount of user generated metadata. These tags represent quite a few different aspects of the resources they describe and it is not obvious whether and how these tags or subsets of them can be used for search. This paper is the ﬁrst to present an in-depth study of tagging behavior for very different kinds of resources and systems –Web pages (Del.icio.us), music (Last.fm), and images (Flickr) –and compares the results with anchor text characteristics. We analyze and classify sample tags from these systems, to get an insight into what kinds of tags are used for different resources, and provide statistics on tag distributions in all three tagging environments. Since even relevant tags may not add new information to the search procedure, we also check overlap of tags with content, with metadata assigned by experts and from other sources. We discuss the potential of different kinds of tags for improving search, comparing them with user queries posted to search engines as well as through a user survey. The results are promising and provide more insight into both the use of different kinds of tags for improving search and possible extensions of tagging systems to support the creation of potentially search-relevant tags.},
	language = {en},
	urldate = {2021-12-05},
	booktitle = {Proceeding of the 17th {ACM} conference on {Information} and knowledge mining - {CIKM} '08},
	publisher = {ACM Press},
	author = {Bischoff, Kerstin and Firan, Claudiu S. and Nejdl, Wolfgang and Paiu, Raluca},
	year = {2008},
	pages = {193},
	file = {Bischoff 等。 - 2008 - Can all tags be used for search.pdf:/home/yin/OneDrive/Zotero/storage/TXKHQWZD/Bischoff 等。 - 2008 - Can all tags be used for search.pdf:application/pdf},
}

@inproceedings{xu_dspr_2016,
author = {Xu, Zhenghua and Chen, Cheng and Lukasiewicz, Thomas and Miao, Yishu and Meng, Xiangwu},
title = {Tag-Aware Personalized Recommendation Using a Deep-Semantic Similarity Model with Negative Sampling},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983874},
doi = {10.1145/2983323.2983874},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1921–1924},
numpages = {4},
keywords = {negative sampling, tag-aware personalized recommendation, deep neural network, deep-semantic similarity},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{xu_hdlpr_2017,
author = {Xu, Zhenghua and Lukasiewicz, Thomas and Chen, Cheng and Miao, Yishu and Meng, Xiangwu},
title = {Tag-Aware Personalized Recommendation Using a Hybrid Deep Model},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {3196–3202},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{wu_survey_2022,
	title = {A {Survey} on {Accuracy}-oriented {Neural} {Recommendation}: {From} {Collaborative} {Filtering} to {Information}-rich {Recommendation}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {A {Survey} on {Accuracy}-oriented {Neural} {Recommendation}},
	url = {http://arxiv.org/abs/2104.13030},
	doi = {10.1109/TKDE.2022.3145690},
	abstract = {Influenced by the great success of deep learning in computer vision and language understanding, research in recommendation has shifted to inventing new recommender models based on neural networks. In recent years, we have witnessed significant progress in developing neural recommender models, which generalize and surpass traditional recommender models owing to the strong representation power of neural networks. In this survey paper, we conduct a systematic review on neural recommender models from the perspective of recommendation modeling with the accuracy goal, aiming to summarize this field to facilitate researchers and practitioners working on recommender systems. Specifically, based on the data usage during recommendation modeling, we divide the work into collaborative filtering and information-rich recommendation: 1) collaborative filtering, which leverages the key source of user-item interaction data; 2) content enriched recommendation, which additionally utilizes the side information associated with users and items, like user profile and item knowledge graph; and 3) temporal/sequential recommendation, which accounts for the contextual information associated with an interaction, such as time, location, and the past interactions. After reviewing representative work for each type, we finally discuss some promising directions in this field.},
	urldate = {2023-02-21},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wu, Le and He, Xiangnan and Wang, Xiang and Zhang, Kun and Wang, Meng},
	year = {2022},
	note = {arXiv:2104.13030 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
	pages = {1--1},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	doi = {10.48550/arXiv.1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{velickovic_graph_2018,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	doi = {10.48550/arXiv.1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10903 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@misc{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv:1806.01261 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/2QCDW5BD/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf},
}

@misc{chen_simple_2020,
	title = {Simple and {Deep} {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/2007.02133},
	abstract = {Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the \{{\textbackslash}em over-smoothing\} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: \{{\textbackslash}em Initial residual\} and \{{\textbackslash}em Identity mapping\}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII .},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Chen, Ming and Wei, Zhewei and Huang, Zengfeng and Ding, Bolin and Li, Yaliang},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02133 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {2666-6510},
	shorttitle = {Graph neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
	doi = {10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	language = {en},
	urldate = {2023-02-24},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	month = jan,
	year = {2020},
	keywords = {Deep learning, Graph neural network},
	pages = {57--81},
	file = {ScienceDirect Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/XJCC7R2U/Zhou et al. - 2020 - Graph neural networks A review of methods and app.pdf:application/pdf},
}

@inproceedings{koren_factorization_2008,
	address = {New York, NY, USA},
	series = {{KDD} '08},
	title = {Factorization meets the neighborhood: a multifaceted collaborative filtering model},
	isbn = {978-1-60558-193-4},
	shorttitle = {Factorization meets the neighborhood},
	url = {https://doi.org/10.1145/1401890.1401944},
	doi = {10.1145/1401890.1401944},
	abstract = {Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task.},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Koren, Yehuda},
	month = aug,
	year = {2008},
	keywords = {collaborative filtering, recommender systems},
	pages = {426--434},
}

@article{gori_itemrank_nodate,
	title = {{ItemRank}: {A} {Random}-{Walk} {Based} {Scoring} {Algorithm} for {Recommender} {Engines}},
	abstract = {Recommender systems are an emerging technology that helps consumers to ﬁnd interesting products. A recommender system makes personalized product suggestions by extracting knowledge from the previous users interactions. In this paper, we present ”ItemRank”, a random–walk based scoring algorithm, which can be used to rank products according to expected user preferences, in order to recommend top–rank items to potentially interested users. We tested our algorithm on a standard database, the MovieLens data set, which contains data collected from a popular recommender system on movies, that has been widely exploited as a benchmark for evaluating recently proposed approaches to recommender system (e.g. [Fouss et al., 2005; Sarwar et al., 2002]). We compared ItemRank with other state-of-the-art ranking techniques (in particular the algorithms described in [Fouss et al., 2005]). Our experiments show that ItemRank performs better than the other algorithms we compared to and, at the same time, it is less complex than other proposed algorithms with respect to memory usage and computational cost too.},
	language = {en},
	author = {Gori, Marco},
	file = {Gori - ItemRank A Random-Walk Based Scoring Algorithm fo.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/6NUIS3M8/Gori - ItemRank A Random-Walk Based Scoring Algorithm fo.pdf:application/pdf},
}

@misc{wu_graph_2022,
	title = {Graph {Neural} {Networks} in {Recommender} {Systems}: {A} {Survey}},
	shorttitle = {Graph {Neural} {Networks} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2011.02260},
	doi = {10.48550/arXiv.2011.02260},
	abstract = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
	month = apr,
	year = {2022},
	note = {arXiv:2011.02260 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/GM9IKAUC/Wu et al. - 2022 - Graph Neural Networks in Recommender Systems A Su.pdf:application/pdf},
}

@misc{hu_ogb-lsc_2021,
	title = {{OGB}-{LSC}: {A} {Large}-{Scale} {Challenge} for {Machine} {Learning} on {Graphs}},
	shorttitle = {{OGB}-{LSC}},
	url = {http://arxiv.org/abs/2103.09430},
	doi = {10.48550/arXiv.2103.09430},
	abstract = {Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks -- link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ .},
	urldate = {2023-02-25},
	publisher = {arXiv},
	author = {Hu, Weihua and Fey, Matthias and Ren, Hongyu and Nakata, Maho and Dong, Yuxiao and Leskovec, Jure},
	month = oct,
	year = {2021},
	note = {arXiv:2103.09430 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/NEN2FZJT/Hu et al. - 2021 - OGB-LSC A Large-Scale Challenge for Machine Learn.pdf:application/pdf},
}

@article{bengio_neural_2003,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {https://jmlr.csail.mit.edu/papers/v3/bengio03a},
	abstract = {A goal of statistical language modeling is to learn the joint
probability function of sequences of words in a language. This is
intrinsically difficult because of the curse of dimensionality:
a word sequence on which the model will be tested is likely to be
different from all the word sequences seen during training.
Traditional but very successful approaches based on n-grams obtain
generalization by concatenating very short overlapping
sequences seen in the training
set.  We propose to fight the curse of dimensionality by
learning a distributed representation for words which allows each
training sentence to inform the model about an exponential number of
semantically neighboring sentences.  The model learns simultaneously
(1) a distributed representation for each word along with (2) the
probability function for word sequences, expressed in terms of these
representations. Generalization is obtained because a sequence of
words that has never been seen before gets high probability if it is
made of words that are similar (in the sense of having a nearby
representation) to words forming an already seen sentence. Training
such large models (with millions of parameters) within a reasonable
time is itself a significant challenge.  We report on experiments
using neural networks for the probability function, showing on two
text corpora that the proposed approach significantly improves on
state-of-the-art n-gram models, and that the proposed approach
allows to take advantage of longer contexts.},
	number = {Feb},
	urldate = {2023-02-26},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	pages = {1137--1155},
	file = {Full Text PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/6LCIJBGU/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:application/pdf},
}

@inproceedings{mnih_fast_2012,
	address = {Madison, WI, USA},
	series = {{ICML}'12},
	title = {A fast and simple algorithm for training neural probabilistic language models},
	isbn = {978-1-4503-1285-1},
	abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.},
	urldate = {2023-02-28},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Mnih, Andriy and Teh, Yee Whye},
	month = jun,
	year = {2012},
	pages = {419--426},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/NJYVTB3S/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf},
}

@phdthesis{2022_gnn_rs,
	type = {博士},
	title = {基于图神经网络的个性化推荐},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDTEMP&filename=1023001839.nh&uniplatform=NZKPT&v=QUpwPQIQYPFqva-0fp7t30wlApsQoOOKKJ9RIaBmyGv66o0mAwd4j5OEacac7f5M},
	abstract = {随着信息技术和电子商务的发展,互联网已经成为国民日常生活不可或缺的一部分,对国民的生活质量产生极大影响。然而,互联网上的海量信息在丰富人们的生活、满足国民日益增长的物质和精神需求的同时,也对用户个性化筛选信息、平台精准投放信息带来了巨大挑战。个性化推荐系统作为大数据时代缓解“信息过载”问题的有效工具,已经成为了支撑互联网智能、部署实现高端高效智能经济的关键技术之一。个性化推荐的核心之一是发掘用户意图。传统的启发式方法和基于矩阵分解的方法只能建模浅层的、隐式的用户意图,难以显式地捕捉更深层次的用户意图。由于推荐场景下的大部分数据本质上都是图结构的(如用户-物品的交互数据、用户的社交网络、物品的知...},
	language = {中文;},
	urldate = {2023-01-19},
	school = {中国科学技术大学},
	author = {吴, 剑灿},
	year = {2022},
	doi = {10.27517/d.cnki.gzkju.2022.000378},
	keywords = {图神经网络, Recommender System, Context-aware Recommender System, Graph Neural Networks, Sampled Softmax Loss, Self-supervised Learning, 情境感知推荐系统, 推荐系统, 自监督学习, 采样软最大化损失函数},
}

@misc{wu_effectiveness_2022,
	title = {On the {Effectiveness} of {Sampled} {Softmax} {Loss} for {Item} {Recommendation}},
	url = {http://arxiv.org/abs/2201.02327},
	doi = {10.48550/arXiv.2201.02327},
	abstract = {Learning objectives of recommender models remain largely unexplored. Most methods routinely adopt either pointwise or pairwise loss to train the model parameters, while rarely pay attention to softmax loss due to the high computational cost. Sampled softmax loss emerges as an efficient substitute for softmax loss. Its special case, InfoNCE loss, has been widely used in self-supervised learning and exhibited remarkable performance for contrastive learning. Nonetheless, limited studies use sampled softmax loss as the learning objective to train the recommender. Worse still, none of them explore its properties and answer "Does sampled softmax loss suit for item recommendation?" and "What are the conceptual advantages of sampled softmax loss, as compared with the prevalent losses?", to the best of our knowledge. In this work, we aim to better understand sampled softmax loss for item recommendation. Specifically, we first theoretically reveal three model-agnostic advantages: (1) mitigating popularity bias, which is beneficial to long-tail recommendation; (2) mining hard negative samples, which offers informative gradients to optimize model parameters; and (3) maximizing the ranking metric, which facilitates top-K performance. Moreover, we probe the model-specific characteristics on the top of various recommenders. Experimental results suggest that sampled softmax loss is more friendly to history and graph-based recommenders (e.g., SVD++ and LightGCN), but performs poorly for ID-based models (e.g., MF). We ascribe this to its shortcoming in learning representation magnitude, making the combination with the models that are also incapable of adjusting representation magnitude learn poor representations. In contrast, the history- and graph-based models, which naturally adjust representation magnitude according to node degree, are able to compensate for the shortcoming of sampled softmax loss.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Wu, Jiancan and Wang, Xiang and Gao, Xingyu and Chen, Jiawei and Fu, Hongcheng and Qiu, Tianyu and He, Xiangnan},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02327 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/PL5P88QS/Wu et al. - 2022 - On the Effectiveness of Sampled Softmax Loss for I.pdf:application/pdf},
}

@misc{chen_bias_2021,
	title = {Bias and {Debias} in {Recommender} {System}: {A} {Survey} and {Future} {Directions}},
	shorttitle = {Bias and {Debias} in {Recommender} {System}},
	url = {http://arxiv.org/abs/2010.03240},
	abstract = {While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, etc. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology ``bias'' is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic. The summary of debiasing methods reviewed in this survey can be found at {\textbackslash}url\{https://github.com/jiawei-chen/RecDebiasing\}.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Chen, Jiawei and Dong, Hande and Wang, Xiang and Feng, Fuli and Wang, Meng and He, Xiangnan},
	month = dec,
	year = {2021},
	note = {arXiv:2010.03240 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/TTDSFAP7/Chen et al. - 2021 - Bias and Debias in Recommender System A Survey an.pdf:application/pdf},
}

@inproceedings{abdollahpouri_connection_2020,
	address = {Virtual Event Brazil},
	title = {The {Connection} {Between} {Popularity} {Bias}, {Calibration}, and {Fairness} in {Recommendation}},
	isbn = {978-1-4503-7583-2},
	url = {https://dl.acm.org/doi/10.1145/3383313.3418487},
	doi = {10.1145/3383313.3418487},
	abstract = {Recently there has been a growing interest in fairness-aware recommender systems including fairness in providing consistent performance across different users or groups of users. A recommender system could be considered unfair if the recommendations do not fairly represent the tastes of a certain group of users while other groups receive recommendations that are consistent with their preferences. In this paper, we use a metric called miscalibration for measuring how a recommendation algorithm is responsive to users’ true preferences and we consider how various algorithms may result in different degrees of miscalibration for different users. In particular, we conjecture that popularity bias which is a wellknown phenomenon in recommendation is one important factor leading to miscalibration in recommendation. Our experimental results using two real-world datasets show that there is a connection between how different user groups are affected by algorithmic popularity bias and their level of interest in popular items. Moreover, we show that the more a group is affected by the algorithmic popularity bias, the more their recommendations are miscalibrated.},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad},
	month = sep,
	year = {2020},
	pages = {726--731},
	file = {Abdollahpouri et al. - 2020 - The Connection Between Popularity Bias, Calibratio.pdf:/Users/yin/Library/CloudStorage/OneDrive-Personal/Zotero/storage/VM8GQJJJ/Abdollahpouri et al. - 2020 - The Connection Between Popularity Bias, Calibratio.pdf:application/pdf},
}
