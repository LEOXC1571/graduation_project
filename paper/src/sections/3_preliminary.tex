\chapter[分子图学习与基于扩散模型的图生成]{分子图学习与基于扩散模型的图生成}
\label{chap:diffusion-based_molgen}

\section{分子图学习}
在图学习中通常有两类监督任务：节点分类/回归和图分类/回归。对于图学习，一个分子可以被抽象为一个图 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$，其中 $|\mathcal{V}| = n$表示节点（原子）的集合，$|\mathcal{E}| = m$ 表示分子中的边（化学键）的集合。我们用$e_i$表示节点 $i$的特征，用$e_{ij}$表示边$(i, j)$的特征。

在节点分类或回归任务中，每个节点$i$都有一个标签或目标$y_i$。任务目标是通过学习，预测未见节点的标签。这个任务可被应用于许多应用，比如识别分子中的功能团或预测各个原子的性质。

此外，在图分类或回归任务中，给定一组图$\{ \mathcal{G}_1, \mathcal{G}_2, ..., \mathcal{G}_N \}$和对应的标签或目标 $\{ y_1, ..., y_N \}$。此时任务目标是根据图的结构和节点边的特征，预测给定图的标签或目标。这个任务被广泛应用于化合物分类或基于结构预测分子性质等。

在这两类任务中，目标是利用监督学习技术训练模型，有效地捕捉图的结构与相关标签/目标之间的关系。现有的图学习方法主要有基于图神经网络的模型和基于Transformer架构的模型。

图神经网络（Graph Neural Networks，简称GNNs），近来在知识图谱、社交网络和药物发现等各个领域引起了广泛的关注。GNNs的核心操作在于将图中节点或边之间的特征进行传递（也称为邻居聚合）。消息传递操作通过聚合节点$i$的邻居节点和边的隐式特征来迭代更新节点$i$自身的隐式特征$e_i$。一般来说，消息传递过程包含多轮迭代，每轮迭代可以被视为对更远距离邻居的消息聚合。假设有$L$轮迭代，第$l$轮迭代会将目标节点的l跳邻居特征注入目标节点的隐式特征。第$l$轮迭代中，消息的传递与聚合可被表示为：
\begin{eqnarray}
    &m_j^{(l)} = {\rm MSG}^{(l)}(e_j^{(l-1)}), \ j \in \mathcal{N}_i, & \\
    &e_i^{(l)} = {\rm AGG}^{(l)}(\{ m_j^{(l)}, \ j \in \mathcal{N}_i \}, \ m_i^{(l)} ), &
\end{eqnarray}
m(l,k) = AGGREGATE(l)({(h(l,k−1)v, h(l,k)v) | v与节点v相邻}，其中m(l,k)v是聚合后的消息，σ(·)是某个激活函数。我们约定h(l,0)v := h(l−1,Kl−1)v。选择AGGREGATE(l)(·)的方式有几种流行的方法，如平均值、最大池化和图注意力机制。对于一次消息传递，其中有一个可训练参数层（即AGGREGATE(l)(·)、W(l)和b(l)内部的参数）。这些参数在迭代l中的Kl个跳数之间是共享的。经过L次消息传递后，最后一次迭代中最后一个跳数的隐藏状态被用作节点的嵌入，即h(L,KL)v，其中v ∈ V。最后，应用READOUT操作来获取图级别的表示。

model: gpt-3.5-t

\section{基于扩散模型的图生成}